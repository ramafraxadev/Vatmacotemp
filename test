import json
import os
import mysql.connector
from dotenv import load_dotenv
from groq import Groq
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from datetime import datetime

# === Initialisation ===
load_dotenv()
client = Groq(api_key=os.getenv("GROQ_API_KEY"))

DATA_FILE = "cleanedjsonv1_data.json"
LOG_FILE = "assistant_log.json"
CACHE_FILE = "cache.json"

# Charger JSON
with open(DATA_FILE, "r", encoding="utf-8") as f:
    data = json.load(f)

# Cr√©er fichier log
if not os.path.exists(LOG_FILE):
    with open(LOG_FILE, "w", encoding="utf-8") as f:
        json.dump([], f, ensure_ascii=False, indent=2)

# Charger cache JSON
if os.path.exists(CACHE_FILE):
    with open(CACHE_FILE, "r", encoding="utf-8") as f:
        cache = json.load(f)
else:
    cache = {}

# Connexion MySQL
def connect_db():
    return mysql.connector.connect(
        host=os.getenv("MYSQL_HOST", "localhost"),
        user=os.getenv("MYSQL_USER", "mafra"),
        password=os.getenv("MYSQL_PASSWORD", "uuuku"),
        database=os.getenv("MYSQL_DATABASE", "assistant_macompta")
    )

# TF-IDF Indexation
texts = [f"{d['title']} {d['text']}" for d in data]
vectorizer = TfidfVectorizer(stop_words=None)
X = vectorizer.fit_transform(texts)

def search_in_json(query, top_k=1):
    vec = vectorizer.transform([query])
    sim = cosine_similarity(vec, X).flatten()
    best = sim.argsort()[::-1][:top_k]

    return [{
        "title": data[i]["title"],
        "url": data[i]["url"],
        "text": data[i]["text"],
        "score": float(sim[i])
    } for i in best]

def split_text(text, size=7000):
    return [text[i:i+size] for i in range(0, len(text), size)]


# === 1) Reformulation (llama 3.1 8B) ===
def ask_llama_reformulate(context):
    parts = split_text(context)
    out = ""

    for idx, part in enumerate(parts, 1):
        prompt = f"""
Reformule fid√®lement ce texte sans rien retirer ni ajouter.
Ne change pas les montants, dates, listes, articles, ni la structure.

(Partie {idx}/{len(parts)}) :
{part}
"""
        completion = client.chat.completions.create(
            model="llama-3.1-8b-instant",
            messages=[
                {"role": "system", "content": "Tu reformules fid√®lement, sans inventer."},
                {"role": "user", "content": prompt}
            ],
            temperature=0,
            max_tokens=1800
        )

        out += completion.choices[0].message.content.strip() + "\n\n"

    return out.strip()


# === 2) R√©ponse contextuelle (llama 3.1 8B) ===
def ask_llama_contextual(query, context):
    prompt = f"""
Tu es l'assistant officiel macompta.fr.

R√©pond UNIQUEMENT dans les domaines :
- comptabilit√©
- fiscalit√©
- micro-entreprise
- facturation
- associations
- gestion

Si la question est hors sujet ‚Üí r√©pond : "Je ne peux r√©pondre qu‚Äôaux questions comptables."

Question : {query}

Contexte extrait du site :
{context[:2000]}
"""

    completion = client.chat.completions.create(
        model="llama-3.1-8b-instant",
        messages=[
            {"role": "system", "content": "Tu ne r√©ponds que sur macompta.fr et la comptabilit√©."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.2,
        max_tokens=700
    )

    return completion.choices[0].message.content.strip()


def display_progressive_text(text, chunk_size=7000):
    chunks = split_text(text, chunk_size)
    for i, chunk in enumerate(chunks, 1):
        print(chunk)
        if i < len(chunks):
            suite = input(f"\n--- Segment {i}/{len(chunks)} --- suite ? (o/n) : ")
            if suite.lower() != "o":
                return
    print("\n‚úì Fin du texte.\n")


# === Enregistrer interactions ===
def save_interaction(q, r, mode, meta=None):

    # JSON
    with open(LOG_FILE, "r", encoding="utf-8") as f:
        logs = json.load(f)

    entry = {
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "question": q,
        "response": r,
        "mode": mode,
        "metadata": meta or {}
    }

    logs.append(entry)

    with open(LOG_FILE, "w", encoding="utf-8") as f:
        json.dump(logs, f, ensure_ascii=False, indent=2)

    # MySQL
    try:
        conn = connect_db()
        cursor = conn.cursor()

        cursor.execute("""
            INSERT INTO interactions (
                timestamp, question, response, mode,
                source_title, source_url, similarity_score,
                output_file, metadata
            )
            VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s)
        """, (
            entry["timestamp"], q, r, mode,
            (meta or {}).get("source_title"),
            (meta or {}).get("source_url"),
            (meta or {}).get("similarity_score"),
            (meta or {}).get("output_file"),
            json.dumps(meta or {}, ensure_ascii=False)
        ))

        conn.commit()
        cursor.close()
        conn.close()

    except Exception as e:
        print("‚ö†Ô∏è MySQL ERROR :", e)


# === Boucle principale ===
def assistant_loop():
    print("=== Assistant macompta.fr (Groq + llama-3.1-8b-instant + cache) ===\n")

    while True:
        query = input("Vous : ").strip()

        if query.lower() in ["quit", "exit", "stop"]:
            break

        # Cache
        if query in cache:
            print("\nüìå R√©ponse depuis le cache :\n")
            print(cache[query])
            continue

        result = search_in_json(query)[0]

        # --- faible match ---
        if result["score"] < 0.2:
            print(f"\n‚ö†Ô∏è Faible correspondance ({result['score']})")

            response = ask_llama_contextual(query, result["text"])
            print(response)

            cache[query] = response
            with open(CACHE_FILE, "w", encoding="utf-8") as f:
                json.dump(cache, f, ensure_ascii=False, indent=2)

            save_interaction(query, response, "contextuel", {
                "source_title": result["title"],
                "similarity_score": result["score"]
            })

            continue

        # --- bonne correspondance ---
        print(f"\nüîé Titre trouv√© : {result['title']} (score {result['score']})")
        print(f"üîó URL : {result['url']}\n")

        reformulated = ask_llama_reformulate(result["text"])
        print("\nüßæ Texte reformul√© :\n")
        display_progressive_text(reformulated)

        filename = result["title"].replace(" ", "_") + "_reformule.txt"
        with open(filename, "w", encoding="utf-8") as f:
            f.write(reformulated)

        print("\nüíæ Sauvegard√© sous :", filename)

        cache[query] = reformulated
        with open(CACHE_FILE, "w", encoding="utf-8") as f:
            json.dump(cache, f, ensure_ascii=False, indent=2)

        save_interaction(query, reformulated, "reformulation", {
            "source_title": result["title"],
            "source_url": result["url"],
            "similarity_score": result["score"],
            "output_file": filename
        })


if __name__ == "__main__":
    assistant_loop()
